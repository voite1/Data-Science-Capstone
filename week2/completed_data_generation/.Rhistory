setwd("C:\\Users\\Aleksey\\Documents\\School\\coursera\\Data Science Capstone\\week2\\completed_data_generation")
##setwd("C:\\Users\\db345c\\Desktop\\Data Science Capstone\\week2")
set.seed(63545)
## The purpose of this function is to download, extract, and save
## the only data needed
getDataFiles <- function() {
# Getting the data
datafile.url <- 'https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip'
# Download file if not exists
if(!file.exists('Coursera-SwiftKey.zip')) {
# Fetch and save the file
download.file(datafile, 'Coursera-SwiftKey.zip')
}
## Unzip and sort downloaded files
if(!file.exists('final')) {
# Unzip downloaded file
unzip('./Coursera-SwiftKey.zip')
# Remove unused directories and files
unlink("./final/de_DE", recursive=TRUE)
unlink("./final/ru_RU", recursive=TRUE)
unlink("./final/fi_FI", recursive=TRUE)
# Clean unused variables
rm(datafile.url)
}
}
## The purpose of tis function is to create a subset of data needed
## for processing
createDataSubset <- function() {
# Check if subset of data is created in 'subset', if so, skipp processing
# and load subset file directly
if(file.exists('subset')) {
# load subset of data
conn <- file("./subset/data_subset.txt", 'rb')
txt_data <- readLines(conn, encoding="UTF-8", skipNul=TRUE)
close(conn)
# convert to ascii
txt_data <- iconv(txt_data, from = "latin1", to = "ASCII")
# remove unused variables
rm(conn)
} else {
# Process original files
# Read blogs file
conn <- file('./final/en_US/en_US.blogs.txt', 'r')
blogs <- readLines(conn, encoding="UTF-8", skipNul=TRUE)
close(conn)
# convert to ascii
blogs <- iconv(blogs, from="UTF-8", to="ASCII")
# Read news file
conn <- file('./final/en_US/en_US.news.txt', 'rb')
news <- readLines(conn, encodin="UTF-8", skipNul=TRUE)
close(conn)
# convert to ascii
news <- iconv(news, from="UTF-8", to="ASCII")
# Read twitter file
conn <- file('./final/en_US/en_US.twitter.txt', 'r')
twitter <- readLines(conn, encoding="latin1", skipNul=TRUE)
close(conn)
# Convert to ascii
twitter <- iconv(twitter, from = "latin1", to = "ASCII")
# Load library to get samples (sed for 3 calls below)
library(caTools, quietly=TRUE)
# get 10% sample of blogs
indx <- sample.split(blogs, SplitRatio= .15, group=NULL)
blogs <- blogs[indx]
# get 10% sample of news
indx <- sample.split(news, SplitRatio = .15, group=NULL)
news <- news[indx]
# get 10% sample of twitter
indx <- sample.split(twitter, SplitRatio= .15, group=NULL)
twitter <- twitter[indx]
# combine three subsets into one
txt_data <- c(blogs, news, twitter)
# create subset directory
dir.create('subset')
# write data to a file
conn<-file("./subset/data_subset.txt")
writeLines(txt_data, conn)
close(conn)
# Remove unused variables
rm(indx, conn, blogs, news, twitter)
}
# return data
txt_data
}
## The purpose of this function is to clean the text
cleanDataSubset <- function(data_subset) {
# Load required libraries
library(tm, quietly=TRUE)
library(SnowballC, quietly=TRUE)
# Convert to lower case
data_subset <- tolower(data_subset)
# Remove ' character
data_subset <- gsub("'", "", data_subset)
# Remove digits
data_subset <- gsub("\\d", "", data_subset)
# Remove text in lines only containing 'na'
data_subset <- gsub("^na$", "", data_subset)
# Remove line containing 'na' only
data_subset <- my_data[data_subset != ""]
# Remove punctuation
data_subset <- gsub("[[:punct:]]", " ", data_subset)
# Remove extra spaces
data_subset <- gsub("[ ]+"," ", data_subset)
}
getDataFiles()
my_data <- createDataSubset()
my_data <- cleanDataSubset(my_data)
