print("hi")
url <- 'https://finances.worldbank.org/api/views/uxhv-3yz2/rows.csv?accessType=DOWNLOAD'
data <- download.file(url)
data <- download.file(url, 'test.csv')
data
data <- read.csv('test.csv', header=T)
data
str(data)
pwd()
getwd()
setwd('C:/Users/Aleksey/Document/School/UW_Data_Visualization/UW_Data_Visualization_200/week2/homework')
setwd('C:\\Users\\Aleksey\\Document\\School\\UW_Data_Visualization\\UW_Data_Visualization_200\\week2\\homework')
url
data = read.csv(url)
data
str(data)
dim(data)
colnames(data)
plot(data$regions)
data$Region
data
data <- complete.cases(data)
dim(data)
data
ll
data = read.csv(url)
data
temp = complete.cases(data)
temp
dim(temp)
len(temp)
length(temp)
lengt(temp=F)temp
length(temp=FALSE)
length(temp="FALSE")
length(temp)
data[complete.cases(data),]
data = data[complete.cases(data),]
dim(data)
data
colnames(data)
data$Country
url = 'https://finances.worldbank.org/api/views/uxhv-3yz2/rows.csv?accessType=DOWNLOAD'
data = read.csv(url)
data$Country[, "Namibia"]
data$Country = "Namibia"
url = 'https://finances.worldbank.org/api/views/uxhv-3yz2/rows.csv?accessType=DOWNLOAD'
data = read.csv(url)
str(data)
data[data$Country = "Namibia",]
data[data$Country == "Namibia",]
namibia <- data[data$Country = "Namibia",]
namibia <- data[data$Country == "Namibia",]
str(namibia)
dim(namibia)
ll
install.packages('AppliedPredictiveModeling')
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
install.packages('caret')
library(caret)
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
xnames <- colnames(concrete)[1:8]
featurePlot(x=training[, xnames], y=training$CompressiveStrength, plot="pairs")
### Exploratory data analysis
setwd("C:\\Users\\Aleksey\\Documents\\School\\coursera\\Data Science Capstone\\week2")
## Setting the seed
set.seed(63545)
############################### GET DATA ####################################################
## Getting the data
datafile <- 'https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip'
## Download file if not exists
if(!file.exists('Coursera-SwiftKey.zip')) {
download.file(datafile, 'Coursera-SwiftKey.zip')
}
## Unzip downloaded file if not already unzipped
if(!file.exists('final')) {
unzip('./Coursera-SwiftKey.zip')
}
############################### LOAD DATA ##################################################
## Read blogs file
conn <- file('./final/en_US/en_US.blogs.txt', 'r')
blogs <- readLines(conn, encoding="UTF-8", skipNul=TRUE)
close(conn)
## Read news file
conn <- file('./final/en_US/en_US.news.txt', 'rb')
news <- readLines(conn, skipNul=TRUE)
close(conn)
## Read twitter file
conn <- file('./final/en_US/en_US.twitter.txt', 'r')
twitter <- readLines(conn, encoding="latin1", skipNul=TRUE)
close(conn)
## Convert twitter data to UTF-8 encoding
twitter <- iconv(twitter, from = "latin1", to = "UTF-8", sub="")
## remove non UTF-8 characters
library(stringi)
twitter <- stri_replace_all_regex(twitter, "\u2019|\u201c|\u201d|u201f|``|`",'"')
## Cleanup data file names and connections
rm(conn, datafile)
############################### BASIC STATS ###############################################
## Count number of words in files and display results to the users
system("wc -w ./final/en_US/en_US.blogs.txt",intern=TRUE)
system("wc -w ./final/en_US/en_US.news.txt",intern=TRUE)
system("wc -w ./final/en_US/en_US.twitter.txt",intern=TRUE)
## Display number of lines in files
length(blogs)
length(news)
length(twitter)
############################### SAMPLING DATA ##############################################
## Reduce the data side by sampling for further analysis
library(caTools)
indx <- sample.split(blogs,SplitRatio= .05, group=NULL)
blogs <- blogs[indx]
indx <- sample.split(news,SplitRatio =.05, group=NULL)
news <- news[indx]
indx <- sample.split(twitter,SplitRatio= .05, group=NULL)
twitter <- twitter[indx]
## cleanup
rm(indx)
############################### CREATE CORPORA #############################################
library(tm)
library(RWeka)
library(slam)
blogs.c <- VCorpus(VectorSource(blogs))
news.c <- VCorpus(VectorSource(news))
twitter.c <- VCorpus(VectorSource(twitter))
## Merge corpora
data.c <- c(blogs.c, news.c, twitter.c)
## cleanup
rm(twitter, news, blogs, twitter.c, blogs.c, news.c)
############################## CLEANUP DATA ################################################
## using functions from tm library
library(tm)
data.c <- tm_map(data.c, stripWhitespace)
data.c <- tm_map(data.c, content_transformer(tolower))
data.c <- tm_map(data.c, removePunctuation)
data.c <- tm_map(data.c, removeNumbers)
library(tm)
library(RWeka)
unigramTokenizer <-function(x) { RWeka::NGramTokenizer(x, RWeka::Weka_control(min = 1, max = 1)) }
tdm <- TermDocumentMatrix(data.c, control=list(tokenize = unigramTokenizer))
### Exploratory data analysis
setwd("C:\\Users\\Aleksey\\Documents\\School\\coursera\\Data Science Capstone\\week2")
## Setting the seed
set.seed(63545)
############################### GET DATA ####################################################
## Getting the data
datafile <- 'https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip'
## Download file if not exists
if(!file.exists('Coursera-SwiftKey.zip')) {
download.file(datafile, 'Coursera-SwiftKey.zip')
}
## Unzip downloaded file if not already unzipped
if(!file.exists('final')) {
unzip('./Coursera-SwiftKey.zip')
}
############################### LOAD DATA ##################################################
## Read blogs file
conn <- file('./final/en_US/en_US.blogs.txt', 'r')
blogs <- readLines(conn, encoding="UTF-8", skipNul=TRUE)
close(conn)
## Read news file
conn <- file('./final/en_US/en_US.news.txt', 'rb')
news <- readLines(conn, skipNul=TRUE)
close(conn)
## Read twitter file
conn <- file('./final/en_US/en_US.twitter.txt', 'r')
twitter <- readLines(conn, encoding="latin1", skipNul=TRUE)
close(conn)
## Convert twitter data to UTF-8 encoding
twitter <- iconv(twitter, from = "latin1", to = "UTF-8", sub="")
## remove non UTF-8 characters
library(stringi)
twitter <- stri_replace_all_regex(twitter, "\u2019|\u201c|\u201d|u201f|``|`",'"')
## Cleanup data file names and connections
rm(conn, datafile)
############################### BASIC STATS ###############################################
## Count number of words in files and display results to the users
system("wc -w ./final/en_US/en_US.blogs.txt",intern=TRUE)
system("wc -w ./final/en_US/en_US.news.txt",intern=TRUE)
system("wc -w ./final/en_US/en_US.twitter.txt",intern=TRUE)
## Display number of lines in files
length(blogs)
length(news)
length(twitter)
############################### SAMPLING DATA ##############################################
## Reduce the data side by sampling for further analysis
library(caTools)
indx <- sample.split(blogs,SplitRatio= .01, group=NULL)
blogs <- blogs[indx]
indx <- sample.split(news,SplitRatio =.01, group=NULL)
news <- news[indx]
indx <- sample.split(twitter,SplitRatio= .01, group=NULL)
twitter <- twitter[indx]
## cleanup
rm(indx)
library(tm)
library(RWeka)
library(slam)
blogs.c <- VCorpus(VectorSource(blogs))
news.c <- VCorpus(VectorSource(news))
twitter.c <- VCorpus(VectorSource(twitter))
## Merge corpora
data.c <- c(blogs.c, news.c, twitter.c)
## cleanup
rm(twitter, news, blogs, twitter.c, blogs.c, news.c)
### Exploratory data analysis
setwd("C:\\Users\\Aleksey\\Documents\\School\\coursera\\Data Science Capstone\\week2")
## Setting the seed
set.seed(63545)
############################### GET DATA ####################################################
## Getting the data
datafile <- 'https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip'
## Download file if not exists
if(!file.exists('Coursera-SwiftKey.zip')) {
download.file(datafile, 'Coursera-SwiftKey.zip')
}
## Unzip downloaded file if not already unzipped
if(!file.exists('final')) {
unzip('./Coursera-SwiftKey.zip')
}
############################### LOAD DATA ##################################################
## Read blogs file
conn <- file('./final/en_US/en_US.blogs.txt', 'r')
blogs <- readLines(conn, encoding="UTF-8", skipNul=TRUE)
close(conn)
## Read news file
conn <- file('./final/en_US/en_US.news.txt', 'rb')
news <- readLines(conn, skipNul=TRUE)
close(conn)
## Read twitter file
conn <- file('./final/en_US/en_US.twitter.txt', 'r')
twitter <- readLines(conn, encoding="latin1", skipNul=TRUE)
close(conn)
## Convert twitter data to UTF-8 encoding
twitter <- iconv(twitter, from = "latin1", to = "UTF-8", sub="")
## remove non UTF-8 characters
library(stringi)
twitter <- stri_replace_all_regex(twitter, "\u2019|\u201c|\u201d|u201f|``|`",'"')
## Cleanup data file names and connections
rm(conn, datafile)
############################### BASIC STATS ###############################################
## Count number of words in files and display results to the users
system("wc -w ./final/en_US/en_US.blogs.txt",intern=TRUE)
system("wc -w ./final/en_US/en_US.news.txt",intern=TRUE)
system("wc -w ./final/en_US/en_US.twitter.txt",intern=TRUE)
## Display number of lines in files
length(blogs)
length(news)
length(twitter)
############################### SAMPLING DATA ##############################################
## Reduce the data side by sampling for further analysis
library(caTools)
indx <- sample.split(blogs,SplitRatio= .005, group=NULL)
blogs <- blogs[indx]
indx <- sample.split(news,SplitRatio =.005, group=NULL)
news <- news[indx]
indx <- sample.split(twitter,SplitRatio= .005, group=NULL)
twitter <- twitter[indx]
## cleanup
rm(indx)
library(tm)
library(RWeka)
library(slam)
blogs.c <- VCorpus(VectorSource(blogs))
news.c <- VCorpus(VectorSource(news))
twitter.c <- VCorpus(VectorSource(twitter))
## Merge corpora
data.c <- c(blogs.c, news.c, twitter.c)
## cleanup
rm(twitter, news, blogs, twitter.c, blogs.c, news.c)
## using functions from tm library
library(tm)
data.c <- tm_map(data.c, stripWhitespace)
data.c <- tm_map(data.c, content_transformer(tolower))
data.c <- tm_map(data.c, removePunctuation)
data.c <- tm_map(data.c, removeNumbers)
library(tm)
library(RWeka)
unigramTokenizer <-function(x) { RWeka::NGramTokenizer(x, RWeka::Weka_control(min = 1, max = 1)) }
tdm <- TermDocumentMatrix(data.c, control=list(tokenize = unigramTokenizer))
words <- as.data.frame(slam::row_sums(tdm, na.rm=T))
colnames(words)<- "Frequency"
words <- cbind(word = rownames(words),words)
rownames(words) <- NULL
dim(words)
str(words)
sorted(words$Frequency)
words$Frequency
tdm2 <- TermDocumentMatrix(data.c, control=list(tokenize = BigramTokenizer))
BigramTokenizer <-function(x) { RWeka::NGramTokenizer(x, RWeka::Weka_control(min = 2, max = 2)) }
tdm2 <- TermDocumentMatrix(data.c, control=list(tokenize = BigramTokenizer))
words2$Frequency
## convert bigram docmatrix to a dataframe with word and freq
words2 <- as.data.frame(slam::row_sums(tdm, na.rm=T))
colnames(words2)<- "Frequency"
words2 <- cbind(word2 = rownames(words2),words2)
rownames(words2) <- NULL
str(words2)
words2$Frequency
builtDataFrame <- function(x) {
## convert docmatrix to a dataframe with word and freq
item <- as.data.frame(slam::row_sums(x, na.rm=T))
colnames(item)<- "Frequency"
item <- cbind(item = rownames(item),item)
rownames(item) <- NULL
item
}
## convert docmatrix to a dataframe with word and freq
builtDataFrame <- function(x) {
item <- as.data.frame(slam::row_sums(x, na.rm=T))
colnames(item)<- "Frequency"
item <- cbind(item = rownames(item),item)
rownames(item) <- NULL
item
}
## convert docmatrix to a dataframe with word and freq
builtDataFrame <- function(x) {
item <- as.data.frame(slam::row_sums(x, na.rm=T))
colnames(item)<- "Frequency"
item <- cbind(item = rownames(item),item)
rownames(item) <- NULL
item
}
tdm <- TermDocumentMatrix(data.c, control=list(tokenize = unigramTokenizer))
## convert docmatrix to a dataframe with word and freq
buildDataFrame <- function(x) {
item <- as.data.frame(slam::row_sums(x, na.rm=T))
colnames(item)<- "Frequency"
item <- cbind(item = rownames(item),item)
rownames(item) <- NULL
item
}
tdm2 <- TermDocumentMatrix(data.c, control=list(tokenize = BigramTokenizer))
tdm3 <- TermDocumentMatrix(data.c, control=list(tokenize = TrigramTokenizer))
TrigramTokenizer <- function(x) { RWeka::NGramTokenizer(x, RWeka::Weka_control(min = 3, max = 3)) }
tdm3 <- TermDocumentMatrix(data.c, control=list(tokenize = TrigramTokenizer))
?save
library(plyr)
words <- arrange(words,desc(Frequency))
head(words)
