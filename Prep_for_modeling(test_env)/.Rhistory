corpus <- tm_map(corpus, toSpace, "/")
corpus <- tm_map(corpus, toSpace, "@")
corpus <- tm_map(corpus, toSpace,"\\|")
corpus <- tm_map(corpus, toSpace, "<")
corpus <- tm_map(corpus, toSpace, ">")
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeWords, c("NA"))
# corpus <- tm_map(corpus, function(x) removeWords(x, stopwords("english")))
# corpus <- tm_map(corpus, stemDocument, language = "english")
corpus <- tm_map(corpus, stripWhitespace)
# Create tokenizers
library(RWeka, quietly=TRUE)
BigramTokenizer <- function(x) { RWeka::NGramTokenizer(x, RWeka::Weka_control(min = 2, max = 2)) }
TrigramTokenizer <- function(x) { RWeka::NGramTokenizer(x, RWeka::Weka_control(min = 3, max = 3)) }
QuadgramTokenizer <- function(x) { RWeka::NGramTokenizer(x, RWeka::Weka_control(min = 4, max = 4)) }
# Create corpus using again, this time using VectorSource
corpus <- Corpus(VectorSource(corpus))
# Create TDM for bi-grams
tdm2 <- TermDocumentMatrix(corpus, control=list(tokenize = BigramTokenizer,
removePunctuation = TRUE,
removeNumbers = TRUE))
setwd('C:\\Users\\db345c\\Desktop\\Data Science Capstone\\Modeling')
############## Download and unzip files ##########################
# Download file if not exists
if(!file.exists('Coursera-SwiftKey.zip')) {
# Getting the data
datafile.url <- 'https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip'
# Fetch and save the file
download.file(datafile.url, 'Coursera-SwiftKey.zip')
# Clean unused variables
rm(datafile.url)
}
## Unzip and sort downloaded files
if(!file.exists('final')) {
# Unzip downloaded file
unzip('./Coursera-SwiftKey.zip')
# Remove unused directories and files
unlink("./final/de_DE", recursive=TRUE)
unlink("./final/ru_RU", recursive=TRUE)
unlink("./final/fi_FI", recursive=TRUE)
}
########## Read original data files in R #########################
conn <- file('./final/en_US/en_US.blogs.txt', 'r')
blogs <- readLines(conn, encoding="UTF-8", skipNul=TRUE)
close(conn)
# Convert to ascii
blogs <- iconv(blogs, from="UTF-8", to="ASCII")
# Read news file
conn <- file('./final/en_US/en_US.news.txt', 'rb')
news <- readLines(conn, encodin="UTF-8", skipNul=TRUE)
close(conn)
# Convert to ascii
news <- iconv(news, from="UTF-8", to="ASCII")
# Read twitter file
conn <- file('./final/en_US/en_US.twitter.txt', 'r')
twitter <- readLines(conn, encoding="latin1", skipNul=TRUE)
close(conn)
# Convert to ascii
twitter <- iconv(twitter, from = "latin1", to = "ASCII")
######### Create a subset of data for analysis ####################
# Load library to get samples (sed for 3 calls below)
library(caTools, quietly=TRUE)
# get 10% sample of blogs
indx <- sample.split(blogs, SplitRatio= .30)
blogs <- blogs[indx]
# get 10% sample of news
indx <- sample.split(news, SplitRatio = .30)
news <- news[indx]
# get 10% sample of twitter
indx <- sample.split(twitter, SplitRatio= .30)
twitter <- twitter[indx]
############# Save subset of files to work with ###################
# create subset directory
if(file.exists('subset')) {
unlink('subset', recursive=TRUE)
dir.create('./subset')
} else {
dir.create('./subset')
}
# write data to a file
conn <- file("./subset/blogs.txt")
writeLines(blogs, conn)
close(conn)
conn <- file("./subset/news.txt")
writeLines(news, conn)
close(conn)
conn <- file("./subset/twitter.txt")
writeLines(twitter, conn)
close(conn)
############## Clean R Workspace  ##################################
# Remove unused variables
rm(indx, conn, blogs, news, twitter)
################## Loading created text document ###################
library(tm, quietly=TRUE)
library(SnowballC, quietly=TRUE)
files <- DirSource("./subset")
corpus <- VCorpus(x=files)
# Clean corpus: function to remove specified patterns
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
# Clean corpus removing text that is not needed
corpus <- tm_map(corpus, toSpace, "/")
corpus <- tm_map(corpus, toSpace, "@")
corpus <- tm_map(corpus, toSpace,"\\|")
corpus <- tm_map(corpus, toSpace, "<")
corpus <- tm_map(corpus, toSpace, ">")
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeWords, c("NA"))
# corpus <- tm_map(corpus, function(x) removeWords(x, stopwords("english")))
# corpus <- tm_map(corpus, stemDocument, language = "english")
corpus <- tm_map(corpus, stripWhitespace)
# Create tokenizers
library(RWeka, quietly=TRUE)
BigramTokenizer <- function(x) { RWeka::NGramTokenizer(x, RWeka::Weka_control(min = 2, max = 2)) }
TrigramTokenizer <- function(x) { RWeka::NGramTokenizer(x, RWeka::Weka_control(min = 3, max = 3)) }
QuadgramTokenizer <- function(x) { RWeka::NGramTokenizer(x, RWeka::Weka_control(min = 4, max = 4)) }
# Create corpus using again, this time using VectorSource
corpus <- Corpus(VectorSource(corpus))
tdm2 <- TermDocumentMatrix(corpus, control=list(tokenize = BigramTokenizer,
removePunctuation = TRUE,
removeNumbers = TRUE))
setwd('C:\\Users\\db345c\\Desktop\\Data Science Capstone\\Modeling')
############## Download and unzip files ##########################
# Download file if not exists
if(!file.exists('Coursera-SwiftKey.zip')) {
# Getting the data
datafile.url <- 'https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip'
# Fetch and save the file
download.file(datafile.url, 'Coursera-SwiftKey.zip')
# Clean unused variables
rm(datafile.url)
}
## Unzip and sort downloaded files
if(!file.exists('final')) {
# Unzip downloaded file
unzip('./Coursera-SwiftKey.zip')
# Remove unused directories and files
unlink("./final/de_DE", recursive=TRUE)
unlink("./final/ru_RU", recursive=TRUE)
unlink("./final/fi_FI", recursive=TRUE)
}
########## Read original data files in R #########################
conn <- file('./final/en_US/en_US.blogs.txt', 'r')
blogs <- readLines(conn, encoding="UTF-8", skipNul=TRUE)
close(conn)
# Convert to ascii
blogs <- iconv(blogs, from="UTF-8", to="ASCII")
# Read news file
conn <- file('./final/en_US/en_US.news.txt', 'rb')
news <- readLines(conn, encodin="UTF-8", skipNul=TRUE)
close(conn)
# Convert to ascii
news <- iconv(news, from="UTF-8", to="ASCII")
# Read twitter file
conn <- file('./final/en_US/en_US.twitter.txt', 'r')
twitter <- readLines(conn, encoding="latin1", skipNul=TRUE)
close(conn)
# Convert to ascii
twitter <- iconv(twitter, from = "latin1", to = "ASCII")
######### Create a subset of data for analysis ####################
# Load library to get samples (sed for 3 calls below)
library(caTools, quietly=TRUE)
# get 10% sample of blogs
indx <- sample.split(blogs, SplitRatio= .20)
blogs <- blogs[indx]
# get 10% sample of news
indx <- sample.split(news, SplitRatio = .20)
news <- news[indx]
# get 10% sample of twitter
indx <- sample.split(twitter, SplitRatio= .20)
twitter <- twitter[indx]
############# Save subset of files to work with ###################
# create subset directory
if(file.exists('subset')) {
unlink('subset', recursive=TRUE)
dir.create('./subset')
} else {
dir.create('./subset')
}
# write data to a file
conn <- file("./subset/blogs.txt")
writeLines(blogs, conn)
close(conn)
conn <- file("./subset/news.txt")
writeLines(news, conn)
close(conn)
conn <- file("./subset/twitter.txt")
writeLines(twitter, conn)
close(conn)
############## Clean R Workspace  ##################################
# Remove unused variables
rm(indx, conn, blogs, news, twitter)
################## Loading created text document ###################
library(tm, quietly=TRUE)
library(SnowballC, quietly=TRUE)
files <- DirSource("./subset")
corpus <- VCorpus(x=files)
# Clean corpus: function to remove specified patterns
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
# Clean corpus removing text that is not needed
corpus <- tm_map(corpus, toSpace, "/")
corpus <- tm_map(corpus, toSpace, "@")
corpus <- tm_map(corpus, toSpace,"\\|")
corpus <- tm_map(corpus, toSpace, "<")
corpus <- tm_map(corpus, toSpace, ">")
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeWords, c("NA"))
# corpus <- tm_map(corpus, function(x) removeWords(x, stopwords("english")))
# corpus <- tm_map(corpus, stemDocument, language = "english")
corpus <- tm_map(corpus, stripWhitespace)
# Create tokenizers
library(RWeka, quietly=TRUE)
BigramTokenizer <- function(x) { RWeka::NGramTokenizer(x, RWeka::Weka_control(min = 2, max = 2)) }
TrigramTokenizer <- function(x) { RWeka::NGramTokenizer(x, RWeka::Weka_control(min = 3, max = 3)) }
QuadgramTokenizer <- function(x) { RWeka::NGramTokenizer(x, RWeka::Weka_control(min = 4, max = 4)) }
# Create corpus using again, this time using VectorSource
corpus <- Corpus(VectorSource(corpus))
tdm2 <- TermDocumentMatrix(corpus, control=list(tokenize = BigramTokenizer,
removePunctuation = TRUE,
removeNumbers = TRUE))
setwd('C:\\Users\\db345c\\Desktop\\Data Science Capstone\\Modeling')
############## Download and unzip files ##########################
# Download file if not exists
if(!file.exists('Coursera-SwiftKey.zip')) {
# Getting the data
datafile.url <- 'https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip'
# Fetch and save the file
download.file(datafile.url, 'Coursera-SwiftKey.zip')
# Clean unused variables
rm(datafile.url)
## Unzip and sort downloaded files
}
if(!file.exists('final')) {
# Unzip downloaded file
unzip('./Coursera-SwiftKey.zip')
# Remove unused directories and files
unlink("./final/de_DE", recursive=TRUE)
unlink("./final/ru_RU", recursive=TRUE)
unlink("./final/fi_FI", recursive=TRUE)
}
########## Read original data files in R #########################
conn <- file('./final/en_US/en_US.blogs.txt', 'r')
blogs <- readLines(conn, encoding="UTF-8", skipNul=TRUE)
close(conn)
# Convert to ascii
blogs <- iconv(blogs, from="UTF-8", to="ASCII")
# Read news file
conn <- file('./final/en_US/en_US.news.txt', 'rb')
news <- readLines(conn, encodin="UTF-8", skipNul=TRUE)
close(conn)
# Convert to ascii
news <- iconv(news, from="UTF-8", to="ASCII")
# Read twitter file
conn <- file('./final/en_US/en_US.twitter.txt', 'r')
twitter <- readLines(conn, encoding="latin1", skipNul=TRUE)
close(conn)
# Convert to ascii
twitter <- iconv(twitter, from = "latin1", to = "ASCII")
######### Create a subset of data for analysis ####################
# Load library to get samples (sed for 3 calls below)
library(caTools, quietly=TRUE)
# get 10% sample of blogs
indx <- sample.split(blogs, SplitRatio= .20)
blogs <- blogs[indx]
# get 10% sample of news
indx <- sample.split(news, SplitRatio = .20)
news <- news[indx]
# get 10% sample of twitter
indx <- sample.split(twitter, SplitRatio= .20)
twitter <- twitter[indx]
############# Save subset of files to work with ###################
# create subset directory
if(file.exists('subset')) {
unlink('subset', recursive=TRUE)
dir.create('./subset')
} else {
dir.create('./subset')
}
# write data to a file
conn <- file("./subset/blogs.txt")
writeLines(blogs, conn)
close(conn)
conn <- file("./subset/news.txt")
writeLines(news, conn)
close(conn)
conn <- file("./subset/twitter.txt")
writeLines(twitter, conn)
close(conn)
############## Clean R Workspace  ##################################
# Remove unused variables
rm(indx, conn, blogs, news, twitter)
################## Loading created text document ###################
library(tm, quietly=TRUE)
library(SnowballC, quietly=TRUE)
files <- DirSource("./subset")
corpus <- VCorpus(x=files)
# Clean corpus: function to remove specified patterns
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
# Clean corpus removing text that is not needed
corpus <- tm_map(corpus, toSpace, "/")
corpus <- tm_map(corpus, toSpace, "@")
corpus <- tm_map(corpus, toSpace,"\\|")
corpus <- tm_map(corpus, toSpace, "<")
corpus <- tm_map(corpus, toSpace, ">")
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeWords, c("NA"))
# corpus <- tm_map(corpus, function(x) removeWords(x, stopwords("english")))
# corpus <- tm_map(corpus, stemDocument, language = "english")
corpus <- tm_map(corpus, stripWhitespace)
# Create tokenizers
library(RWeka, quietly=TRUE)
BigramTokenizer <- function(x) { RWeka::NGramTokenizer(x, RWeka::Weka_control(min = 2, max = 2)) }
TrigramTokenizer <- function(x) { RWeka::NGramTokenizer(x, RWeka::Weka_control(min = 3, max = 3)) }
QuadgramTokenizer <- function(x) { RWeka::NGramTokenizer(x, RWeka::Weka_control(min = 4, max = 4)) }
# Create corpus using again, this time using VectorSource
corpus <- Corpus(VectorSource(corpus))
# Create TDM for bi-grams
tdm2 <- TermDocumentMatrix(corpus, control=list(tokenize = BigramTokenizer,
removePunctuation = TRUE,
removeNumbers = TRUE))
setwd('C:\\Users\\db345c\\Desktop\\Data Science Capstone\\Modeling')
############## Download and unzip files ##########################
# Download file if not exists
if(!file.exists('Coursera-SwiftKey.zip')) {
# Getting the data
datafile.url <- 'https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip'
# Fetch and save the file
download.file(datafile.url, 'Coursera-SwiftKey.zip')
# Clean unused variables
rm(datafile.url)
}
## Unzip and sort downloaded files
if(!file.exists('final')) {
# Unzip downloaded file
unzip('./Coursera-SwiftKey.zip')
# Remove unused directories and files
unlink("./final/de_DE", recursive=TRUE)
unlink("./final/ru_RU", recursive=TRUE)
unlink("./final/fi_FI", recursive=TRUE)
}
########## Read original data files in R #########################
conn <- file('./final/en_US/en_US.blogs.txt', 'r')
blogs <- readLines(conn, encoding="UTF-8", skipNul=TRUE)
close(conn)
# Convert to ascii
blogs <- iconv(blogs, from="UTF-8", to="ASCII")
# Read news file
conn <- file('./final/en_US/en_US.news.txt', 'rb')
news <- readLines(conn, encodin="UTF-8", skipNul=TRUE)
close(conn)
# Convert to ascii
news <- iconv(news, from="UTF-8", to="ASCII")
# Read twitter file
conn <- file('./final/en_US/en_US.twitter.txt', 'r')
twitter <- readLines(conn, encoding="latin1", skipNul=TRUE)
close(conn)
# Convert to ascii
twitter <- iconv(twitter, from = "latin1", to = "ASCII")
######### Create a subset of data for analysis ####################
# Load library to get samples (sed for 3 calls below)
library(caTools, quietly=TRUE)
# get 10% sample of blogs
indx <- sample.split(blogs, SplitRatio= .17)
blogs <- blogs[indx]
# get 10% sample of news
indx <- sample.split(news, SplitRatio = .17)
news <- news[indx]
# get 10% sample of twitter
indx <- sample.split(twitter, SplitRatio= .17)
twitter <- twitter[indx]
############# Save subset of files to work with ###################
# create subset directory
if(file.exists('subset')) {
unlink('subset', recursive=TRUE)
dir.create('./subset')
} else {
dir.create('./subset')
}
# write data to a file
conn <- file("./subset/blogs.txt")
writeLines(blogs, conn)
close(conn)
conn <- file("./subset/news.txt")
writeLines(news, conn)
close(conn)
conn <- file("./subset/twitter.txt")
writeLines(twitter, conn)
close(conn)
############## Clean R Workspace  ##################################
# Remove unused variables
rm(indx, conn, blogs, news, twitter)
################## Loading created text document ###################
library(tm, quietly=TRUE)
library(SnowballC, quietly=TRUE)
files <- DirSource("./subset")
corpus <- VCorpus(x=files)
# Clean corpus: function to remove specified patterns
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
# Clean corpus removing text that is not needed
corpus <- tm_map(corpus, toSpace, "/")
corpus <- tm_map(corpus, toSpace, "@")
corpus <- tm_map(corpus, toSpace,"\\|")
corpus <- tm_map(corpus, toSpace, "<")
corpus <- tm_map(corpus, toSpace, ">")
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeWords, c("NA"))
# corpus <- tm_map(corpus, function(x) removeWords(x, stopwords("english")))
# corpus <- tm_map(corpus, stemDocument, language = "english")
corpus <- tm_map(corpus, stripWhitespace)
# Create tokenizers
library(RWeka, quietly=TRUE)
BigramTokenizer <- function(x) { RWeka::NGramTokenizer(x, RWeka::Weka_control(min = 2, max = 2)) }
TrigramTokenizer <- function(x) { RWeka::NGramTokenizer(x, RWeka::Weka_control(min = 3, max = 3)) }
QuadgramTokenizer <- function(x) { RWeka::NGramTokenizer(x, RWeka::Weka_control(min = 4, max = 4)) }
# Create corpus using again, this time using VectorSource
corpus <- Corpus(VectorSource(corpus))
tdm2 <- TermDocumentMatrix(corpus, control=list(tokenize = BigramTokenizer,
removePunctuation = TRUE,
removeNumbers = TRUE))
tdm3 <- TermDocumentMatrix(corpus, control=list(tokenize = TrigramTokenizer,
removePunctuation = TRUE,
removeNumbers = TRUE))
bigrams <- read.csv("./data_frames/bigrams.csv")
trigrams <- read.csv("./data_frames/trigrams.csv")
quadgrams <- read.csv("./data_frames/quadgrams.csv")
rm(ls=())
rm(ls)
library(stringi)
w1 <- word(bigrams$item, 1)
library(stringr)
w1 <- word(bigrams$item, 1)
head(wd1)
head(w1)
tail(w1)
head(bigrams)
w2 < -word(bigrams$item, 2)
w2 <- word(bigrams$item, 2)
head(bigrams)
head(w1)
head(w2)
prediction <- word(bigrams$item, 2)
bigrams1 <- data.frame(w1=w1, prediction=prediction, rank=bigrams$Frequency)
head(bigrams)
head(bigrams1)
w1 <- word(trigrams$item, 1)
w2 <- word(trigrams$item, 2)
prediction <- word(trigrams$item, 3)
trigrams1 <- data.frame(w1=w1, prediction=prediction, rank=bigrams$Frequency)
trigrams1 <- data.frame(w1=w1, w2=w2, prediction=prediction, rank=bigrams$Frequency)
trigrams1 <- data.frame(w1=w1, w2=w2, prediction=prediction, rank=trigrams$Frequency)
head(trigrams)
head(trigrams1)
# Update quadgrams data frame
w1 <- word(quadgrams$item, 1)
w2 <- word(quadgrams$item, 2)
w3 <- word(quadgrams$item, 3)
prediction <- word(quadgrams$item, 4)
quadgrams1 <- data.frame(w1=w1, w2=w2, , w3=w3, prediction=prediction, rank=quadgrams$Frequency)
quadgrams1 <- data.frame(w1=w1, w2=w2, w3=w3, prediction=prediction, rank=quadgrams$Frequency)
head(quadgrams)
head(quadgrams1)
bigrams <- read.csv("./data_frames/bigrams.csv")
trigrams <- read.csv("./data_frames/trigrams.csv")
quadgrams <- read.csv("./data_frames/quadgrams.csv")
library(stringr)
# Update bigrams data frame
w1 <- word(bigrams$item, 1)
prediction <- word(bigrams$item, 2)
bigrams <- data.frame(w1=w1, prediction=prediction, rank=bigrams$Frequency)
# Update trigrams data frame
w1 <- word(trigrams$item, 1)
w2 <- word(trigrams$item, 2)
prediction <- word(trigrams$item, 3)
trigrams <- data.frame(w1=w1, w2=w2, prediction=prediction, rank=trigrams$Frequency)
# Update quadgrams data frame
w1 <- word(quadgrams$item, 1)
w2 <- word(quadgrams$item, 2)
w3 <- word(quadgrams$item, 3)
prediction <- word(quadgrams$item, 4)
quadgrams <- data.frame(w1=w1, w2=w2, w3=w3, prediction=prediction, rank=quadgrams$Frequency)
# create subset directory
if(file.exists('data_frames_ready')) {
unlink('data_frames_ready', recursive=TRUE)
dir.create('./data_frames_ready')
} else {
dir.create('./data_frames_ready')
}
# write data to a file
write.csv(bigramDF, "./data_frames_ready/bigrams.csv", row.names=FALSE)
write.csv(bigrams, "./data_frames_ready/bigrams.csv", row.names=FALSE)
write.csv(trigrams, "./data_frames_ready/trigrams.csv", row.names=FALSE)
write.csv(quadgrams, "./data_frames_ready/quadgrams.csv", row.names=FALSE)
rm(prediction, w1, w2, w3)
save(list = ls(all.names = TRUE), file = "dataframes.RData", envir = .GlobalEnv)
save(list = ls(all.names = TRUE), file = "./data_frames_ready/dataframes.RData", envir = .GlobalEnv)
load("./data_frames_ready/dataframes.RData", envir = .GlobalEnv, verbose = FALSE)
load("./data_frames_ready/dataframes.RData", envir = .GlobalEnv, verbose = FALSE)
load("./data_frames_ready/dataframes.RData", envir = parent.frame(), verbose = FALSE)
rm(list=ls())
load("./data_frames_ready/dataframes.RData", envir = .GlobalEnv, verbose = FALSE)
# The line above also works
rm(list=ls())
load("./data_frames_ready/dataframes.RData", envir = parent.frame(), verbose = FALSE)
