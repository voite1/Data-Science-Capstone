print("hi")
url <- 'https://finances.worldbank.org/api/views/uxhv-3yz2/rows.csv?accessType=DOWNLOAD'
data <- download.file(url)
data <- download.file(url, 'test.csv')
data
data <- read.csv('test.csv', header=T)
data
str(data)
pwd()
getwd()
setwd('C:/Users/Aleksey/Document/School/UW_Data_Visualization/UW_Data_Visualization_200/week2/homework')
setwd('C:\\Users\\Aleksey\\Document\\School\\UW_Data_Visualization\\UW_Data_Visualization_200\\week2\\homework')
url
data = read.csv(url)
data
str(data)
dim(data)
colnames(data)
plot(data$regions)
data$Region
data
data <- complete.cases(data)
dim(data)
data
ll
data = read.csv(url)
data
temp = complete.cases(data)
temp
dim(temp)
len(temp)
length(temp)
lengt(temp=F)temp
length(temp=FALSE)
length(temp="FALSE")
length(temp)
data[complete.cases(data),]
data = data[complete.cases(data),]
dim(data)
data
colnames(data)
data$Country
url = 'https://finances.worldbank.org/api/views/uxhv-3yz2/rows.csv?accessType=DOWNLOAD'
data = read.csv(url)
data$Country[, "Namibia"]
data$Country = "Namibia"
url = 'https://finances.worldbank.org/api/views/uxhv-3yz2/rows.csv?accessType=DOWNLOAD'
data = read.csv(url)
str(data)
data[data$Country = "Namibia",]
data[data$Country == "Namibia",]
namibia <- data[data$Country = "Namibia",]
namibia <- data[data$Country == "Namibia",]
str(namibia)
dim(namibia)
ll
install.packages('AppliedPredictiveModeling')
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
install.packages('caret')
library(caret)
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
xnames <- colnames(concrete)[1:8]
featurePlot(x=training[, xnames], y=training$CompressiveStrength, plot="pairs")
install.packages('tm')
install.packages('SnowballC')
install.packages('worldcloud')
install.packages('wordcloud')
install.packages('e1071')
install.packages('caret')
install.packages('RTextTools')
install.packages("topicmodels")
install.packages('slam')
install.packages('RKEA')
setwd("C:\\Users\\Aleksey\\Documents\\School\\coursera\\Data Science Capstone\\test")
library(tm)
library(SnowballC)
library(wordcloud)
library(e1071)
library(caret)
library(RTextTools)
library(topicmodels)
library(slam)
library(RKEA)
texts = read.csv("text_messages.csv", stringsAsFactors = FALSE)
setwd("C:\\Users\\Aleksey\\Documents\\School\\coursera\\Data Science Capstone\\test")
library(tm)
library(SnowballC)
library(wordcloud)
library(e1071)
library(caret)
library(RTextTools)
library(topicmodels)
library(slam)
library(RKEA)
##-----Load Data Sets----- PAGE 8 of the slides
# Load email list
texts = read.csv("text_messages.csv", stringsAsFactors = FALSE)
texts = read.csv("text_messages.csv", stringsAsFactors = FALSE)
# Change to lower case:
texts$Message = tolower(texts$Message)
# Remove punctuation
texts$Message = sapply(texts$Message, function(x) gsub("'", "", x))
# Now the rest of the punctuation
texts$Message = sapply(texts$Message, function(x) gsub("[[:punct:]]", " ", x))
# Remove numbers
texts$Message = sapply(texts$Message, function(x) gsub("\\d","",x))
# Remove extra white space, so we can split words by spaces
texts$Message = sapply(texts$Message, function(x) gsub("[ ]+"," ",x))
# Remove non-ascii
texts$Message = iconv(texts$Message, from="latin1", to="ASCII", sub="")
# remove stopwords
stopwords()
my_stops = as.character(sapply(stopwords(), function(x) gsub("'","",x)))
texts$Message = sapply(texts$Message, function(x){
paste(setdiff(strsplit(x," ")[[1]],stopwords()),collapse=" ")
})
# Remove extra white space again:
texts$Message = sapply(texts$Message, function(x) gsub("[ ]+"," ",x))
# Stem words:
texts$message_stem = sapply(texts$Message, function(x){
paste(setdiff(wordStem(strsplit(x," ")[[1]]),""),collapse=" ")
})
View(texts)
View(texts)
text_corpus = Corpus(VectorSource(texts$message_stem))
# Build a Document Term Matrix
# Terms        Docs
#            ... 32 33 34 35 36 37 38 39 ...
# about       0  0  1  1  1  0  1  2  1  0
# wut         0  0  0  0  0  0  1  0  0  0
# lol         0  0  0  1  0  0  0  0  0  0
# u           0  0  0  0  0  0  0  0  0  0
# rofl        0  0  1  0  0  0  0  0  0  0
# ...
text_term_matrix = DocumentTermMatrix(text_corpus)
dim(text_term_matrix)
# Save Matrix (This is mostly empty)
text_corpus_mat = as.matrix(text_term_matrix)
dim(text_corpus_mat)
# Convert to Data Frame
text_frame = as.data.frame(text_corpus_mat)
# Use only the most common terms
which_cols_to_use = which(colSums(text_corpus_mat) > 10)
text_frame = text_frame[,which_cols_to_use]
# Convert to factors: (to use with naive base)
text_frame = as.data.frame(lapply(text_frame, as.factor))
# Add the response
text_frame$type = texts$Type
# Split into train/test set
train_ind = sample(1:nrow(text_frame), round(0.8*nrow(text_frame)))
train_set = text_frame[train_ind,]
test_set = text_frame[-train_ind,]
# Compute Naive Bayes Model
text_nb = naiveBayes(as.factor(type) ~ ., data = train_set)
test_predictions = predict(text_nb, newdata = test_set, type="class")
test_predictions
confusionMatrix(test_predictions, as.factor(test_set$type))
important_words = setdiff(names(text_frame), "type")
sample_text = "Please call asap for free consultation!"
sample_text = tolower(sample_text)
sample_text = gsub("'", "", sample_text)
sample_text = gsub("[[:punct:]]", " ", sample_text)
sample_text = gsub("\\d","",sample_text)
sample_text = gsub("[ ]+"," ",sample_text)
sample_text = iconv(sample_text, from="latin1", to="ASCII", sub="")
sample_text = gsub("[ ]+"," ",sample_text)
sample_text = paste(setdiff(wordStem(strsplit(sample_text," ")[[1]]),""),collapse=" ")
# Create occurence vector of important words
sample_occurences = sapply(important_words, function(x){
return(as.numeric(x%in%strsplit(sample_text," ")[[1]]))
})
sample_data = as.data.frame(t(sample_occurences))
sample_prediction = predict(text_nb, newdata = sample_data, type = "class")
sample_prediction
